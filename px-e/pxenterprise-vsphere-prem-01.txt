# /var/k8s mount to network share
# use, modify XX-cluster-ingress-grafana.yaml, XX-cluster-ingress-prometheus.yaml
#  spec.rules.host: px-grafana-XX.testco.local
#  spec.rules.host: px-metrics-XX.testco.local

## from CONTROL PLANE NODE review, satisfy cluster(s) pre-requisites re: Kubernetes version
k version --short

# from EACH WORKER NODE, confirm PX dedicated storage
lsblk
#   70G /dev/sdb specified as KVDB metadata drive
#  400G /dev/sdc specified as PX volume

# from CONTROL PLANE NODE label nodes for PX KVDB device with px/metadata-node=true
k label nodes or-cluster02-w01 or-cluster02-w02 or-cluster02-w03 or-cluster02-w04  px/metadata-node=true

## from CENTRAL.PORTWORX.COM, create and save Portworx Operator and StorageCluster specs
# Select Platform: vSphere, Customize
#  Note: default Portworx vSphere platform behavior is to dynamically provision and manage storage using vSphere's API.
# Basic
#  Use the Portworx Operator [selected]
# PX version [latest]
# k8s version [kubectl version]
# namespace kube-system
# etcd built-in
# OnPremises
# manually specified disks
# drive/device: /dev/sdc
# auto create journal device [selected]
# skip kvdb device [not selected]  /dev/sdb
# data network interfaces: auto
# management network interface: auto
# advanced, starting port for Portworx services: 9001
# customize none
# environment variables none
# registry and image settings 
# security settings enabling authorization [not selected]
# enable stork, CSI, monitoring, telemetry
# cluster name prefix px-cluster
# secrets store type kubernetes
# Spec Name: or-cluster01Spec1
# Spec Tags: or-cluster01

# [If needed] Create kube config for root
sudo -i
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# from CONTROL PLANE NODE deploy Portworx Operator spec
# AS ROOT
kubectl apply -f 'https://install.portworx.com/2.13?comp=pxoperator&kbver=1.23.16&ns=kube-system'

# from CONTROL PLANE NODE deploy StorageCluster spec
# AS ROOT
kubectl apply -f https://install.portworx.com/?operator=true&mc=false&kbver=1.23.16&ns=kube-system&b=true&s=%2Fdev%2Fsdc&j=auto&kd=%2Fdev%2Fsdb&c=px-cluster-c0153345-a7ac-4a47-bb7c-b85eada65157&stork=true&csi=true&mon=true&tel=true&st=k8s&promop=true

# from CONTROL PLANE NODE verify StorageCluster created
k get storagecluster -n kube-system

# from CONTROL PLANE NODE verify StorageNodes status
k get storagenodes -n kube-system

# from CONTROL PLANE NODE verify all PX pods running
k get pods -n kube-system -o wide | grep -e portworx -e px

# from CONTROL PLANE verify PX StorageClass objects
k get sc

# from CONTROL PLANE NODE use script to create pxctl alias
# Node: if \r error encountered, install then sudo dos2unix file
cd ~/px
vi pxctl.sh
  PX_POD=$(kubectl get pods -l name=portworx -n kube-system -o jsonpath='{.items[0].metadata.name}')
  alias pxctl='kubectl exec $PX_POD -n kube-system -- /opt/pwx/bin/pxctl'
source pxctl.sh

# from CONTROL PLANE NODE verify pxctl cluster status on a storage node
pxctl status

# from CONTROL PLANE NODE verify pxctl display alerts from a storage node
pxctl alerts show

# from CONTROL PLANE NODE verify OSB is up and running on a PX node
kubectl exec $PX_POD -n kube-system -- curl -k https://pxessentials.portworx.com/osb/ping

# from EACH WORKER NODE, confirm PX volume changes
lsblk

# from CONTROL PLANE NODE create, describe StorageClass
k apply -f sc-local-01.yaml
k describe sc sc-local-01

# from CONTROL PLANE NODE create, describe PVC using StorageClass; note PX volume created by PVC
k apply -f pvc-local-01.yaml
k describe pvc pvc-local-01

# from CONTROL PLANE NODE get pool status from a PX node
pxctl service pool show

# In vSphere provisioned storage, list PX disks, datastores, VMDKs
pxctl clouddrive list

# from CONTROL PLANE NODE get volume list from a PX node
pxctl volume list

# from CONTROL PLANE NODE inspect specific volume from a PX node
pxctl volume inspect pvc-e2d8aeb9-9eef-4af2-9774-f455dab689ce

# from CONTROL PLANE NODE inspect PX cluster nodes; get node IDs from pxtcl status
pxctl cluster inspect c262c048-90ed-4eb6-8a08-793918b94148
pxctl cluster inspect 916423b5-c7bb-4f58-9214-72acb476cd87
pxctl cluster inspect 0c307cfa-f97f-4810-b66c-13ce79781c40

# from CONTROL PLANE NODE describe px-prometheus Service
k describe svc -n kube-system px-prometheus

# from CONTROL PLANE NODE, create Ingress controller
# from CONTROL PLANE NODE create, describe a cluster specific ingress-px-prometheus Ingress object
k apply -f XX-cluster-ingress-prometheus.yaml
k describe ingress -n kube-system ingress-px-prometheus

# from outside cluster, confirm Prometheus UI access via px-metrics-01.testco.local:[NodePort of ingress controller]
http://px-metrics-03.testco.local:32212
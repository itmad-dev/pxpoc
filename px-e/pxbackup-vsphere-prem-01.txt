# /var/k8s mount to network share
# PX-Enterprise cluster running; see pxenterprise-vsphere-prem-01.sh or equivalent
# use, modify XX-cluster-ingress-grafana.yaml, XX-cluster-ingress-prometheus.yaml, XX-cluster-ingress-px-backup.yaml
#  spec.rules.host: px-grafana-XX.testco.local
#  spec.rules.host: px-metrics-XX.testco.local
#  spec.rules.host: px-backup-XX.testco.local

##  installation pre-requisites
Note: https://backup.docs.portworx.com/install/on-premise/install-prereq/

# [If needed] install Helm
sudo snap install helm --classic

# dns host records – testco.local
px-backup-01	10.1.6.115
px-central-01	10.1.6.115
px-grafana-01	10.1.6.115
px-metrics-01	10.1.6.115
qed-cluster-01	10.1.6.115


## px-central on-premises, px-backup component enabled

# create portworx-sc StorageClass
k apply -f portworx-sc.yaml

# from https://central.portworx.com/specGen/px-backup-wizard, create and save spec to install namespace, PX-Central, and enable PX-Backup
# namespace: central
# install using helm 3
# on-premises
# storage class name: portworx-sc
# use your OIDC [not selected]
# use custom registry [not selected]

# add portworx repository to portworx helm path, update local help repo
helm repo add portworx http://charts.portworx.io/ && helm repo update

# modify cluster-specific local values-px-central.yaml from LoadBalancer to NodePort
# see lab-pxbackup-vsphere-prem-01.docx or equivalent

kubectl create ns central
# place public key cert.pem cert file locally 
kubectl -n central create secret generic px-s3-certs --from-file=cert.pem

# run px-central helm install against modifed cluster-specific local values-px-central.yaml
# creates and installs to central namespace
# helm install px-central portworx/px-central --namespace central  --version 2.3.3 -f values-px-central.yaml --set caCertsSecretName=px-s3-certs


# monitor readiness of central namespace all objects, pvcs
# Note in particular status of pod/pxcentral-post-install-hook-xxxxx which may take 15+ minutes to get to Completed status
k get all,pvc -n central -o wide

## preliminary NodePort connection, review of PX-Backup UI
# confirm NodePort configured for service/px-backup-ui
k describe svc -n central px-backup-ui

# confirm access from outside the cluster to PX-Backup UI
http://[worker node IP]:[NodePort]

# change password to activate account
# update user profile email, first/last names to activate account
# Note: see values-px-central.yaml oidc section for preliminary default user with px-backup-infra-admin role assignment

# review default user profile details
http://[worker node IP]:[NodePort]/profile

# Name, account, account provider
# Org ID
# Group(s)
# PX-Central Role(s)
# PX-Backup Role(s)

# review service status
http://[worker node IP]:[NodePort]/dashboard/status

# list of services, status, number of pods deployed
# PX-Central API
# PX-Central-FrontEnd
# PX-Central-Backend
# PX-Backup
# PX-Central-Onprem
# PX-Central-Auth-Service

# px-backup-security – review default role mapping, roles
http://[worker node IP]:[NodePort]/pxBackup/rbac

# review license status
http://[worker node IP]:[NodePort]/license

## preliminary NodePort connection, review of PX-Central UI
# confirm NodePort configured for service/px-central-ui
k describe svc -n central px-central-ui

# confirm access from outside the cluster to PX-Central UI
http://[worker node IP]:[NodePort]


## configure, confirm hostname Ingress access from outside the cluster to PX-Backup UI
# create cluster specific Ingress object for to PX-Backup
k apply -f XX-cluster-ingress-px-backup.yaml
k describe ingress -n central ingress-px-backup-ui

# confirm access from outside the cluster
http://px-backup-XX.testco.local:[NodePort]


## validate Prometheus components and basic operations
# verify Prometheus pods and services running
k get pods -n portworx -A -o wide | grep -i prometheus

# set up Alertmanager
kubectl -n portworx create secret generic alertmanager-portworx --from-file=alertmanager.yaml=alertmanager.yaml

# edit Portworx StorageCluster to enable Alert Manager
#  spec.monitoring.prometheus.alertManager.enabled = true
k get stc -A
kubectl -n portworx edit stc px-cluster-[xxx-xxx-xxx-xxx-xxx]

# confirm AlertManager pods/containers running
k get po -A | grep alert

# access the AlertManager UI
# access the Alertmanager UI through temporary port-forward
k -n portworx port-forward service/alertmanager-portworx --address=10.1.6.110 9093:9093

# confirm AlertManager cluster status
http://10.1.6.110:9093/#/status

# review ingress-px-prometheus
# if needed, create ingress-px-prometheus
#   k apply -f XX-cluster-ingress-prometheus.yaml
k describe ingress -n portworx ingress-px-prometheus

# access the Prometheus UI through temporary port-forward
k -n portworx port-forward service/px-prometheus --address=10.1.6.150 9090:9090

# view provided Prometheus rules
k -n portworx get prometheusrules

# save the Prometheus rules to a prometheusrules.yaml file

## validate Grafana components and basic operations
# download grafana-dashboard-config.yaml
# create grafana-dashboard-config ConfigMap – Note: leave apiVersion at 1, not v1
k -n portworx create configmap grafana-dashboard-config --from-file=grafana-dashboard-config.yaml

# download grafana-datasource.yaml
curl -o https://docs.portworx.com/samples/k8s/pxc/grafana-datasource.yaml

# create Grafana datasource ConfigMap from downloaded yaml file
k -n portworx create configmap grafana-source-config --from-file=grafana-datasource.yaml

# download, install Grafana dashboard json files
curl "https://docs.portworx.com/samples/k8s/pxc/portworx-cluster-dashboard.json" -o portworx-cluster-dashboard.json && curl "https://docs.portworx.com/samples/k8s/pxc/portworx-node-dashboard.json" -o portworx-node-dashboard.json && curl "https://docs.portworx.com/samples/k8s/pxc/portworx-volume-dashboard.json" -o portworx-volume-dashboard.json && curl "https://docs.portworx.com/samples/k8s/pxc/portworx-performance-dashboard.json" -o portworx-performance-dashboard.json && curl "https://docs.portworx.com/samples/k8s/pxc/portworx-etcd-dashboard.json" -o portworx-etcd-dashboard.json

# create grafana-dashboard ConfigMap for downloaded Grafana dashboard json files
kubectl -n portworx create configmap grafana-dashboards --from-file=portworx-cluster-dashboard.json --from-file=portworx-performance-dashboard.json --from-file=portworx-node-dashboard.json --from-file=portworx-volume-dashboard.json --from-file=portworx-etcd-dashboard.json

# apply grafana.yaml modfied from https://docs.portworx.com/samples/k8s/pxc/grafana.yaml
#  portworx namespace
k apply -f grafana_portworx.yaml

# check status of grafana pods
k get po -A | grep grafana

# create cluster specific grafana ingress
k apply -f XX-cluster-ingress-grafana.yaml

# check status of grafana ingress
k describe -n portworx ingress ingress-grafana

# confirm grafana login from outside the cluster using hostname px-grafana-03.testco.local and NodePort
http://px-grafana-03.testco.local:32468

# review grafana
# Portworx cluster dashboard
General, Dashboards, Portworx Cluster Dashboard

# Portworx node dashboard
General, Dashboards, Portworx Node Dashboard

# install Node Exporter
# required for Grafana to measure various machine resources such as, memory, disk, and CPU utilization
# Note: namespace same as where Prometheus, Grafana deployed
k apply -f node-exporter.yaml

# create a NodeExporter service
# Note: namespace same as where Prometheus, Grafana deployed
k apply -f node-exporter-svc.yaml

# create a Service Monitor - modified to inlclude namespace where Prometheus, Grafana deployed
# The serviceMonitorSelector object is automatically appended to the prometheus object that is deployed by the Portworx Operator. The ServiceMonitor will match any serviceMonitor that has the key prometheus and value of portworx or backup
# view dashboards in Grafana with Node Exporter values
# Portworx Performance Dashboard: System – Storage IO Rate; System Network Send/Received; Volume Latency, IOPS, Throughput
k apply -f node-exporter-svcmonitor.yaml

## Lighthouse
# add PX Cluster  http://px-backup-XX.testco.local:[NodePort]/lh/addCluster
# PX Cluster Details
#   cluster name: px-cluster-[50811061]-[20fa]-[4149]-[8f01]-[517f7c39eccc] # from pxctl status - Cluster Summary
#   cluster endpoint: [node]:9001-9021 # verify; cluster UUID autofills [1e6398f4]-[294e]-[433c]-[8cf2]-[789b2a8df7cd] # from pxctl status - Cluster Summary
#   metrics url: 10.11.230:83:3000   # IP value from service/grafana
# Cluster Security
#   PX Security: none

# review nodes, volumes, pools, drives
# all nodes or selected nodes; click node again to de-select
# select node Node info for node details
# select volume for volume details
# select View Volume Analyzer for hierarchical levels of volume performance tiers, file/folder/subfolder: names, modified time, size
# refreshable, real time data


## create/configure S3 bucket including object lock wasabi – https://console.wasabisys.com/#/file_manager
# create s3 bucket
# name: px-testcolab-01
# region: Oregon us-west-1
# Bucket versioning: [selected]
# Bucket Logging: [not selected]
# Enable Object Locking: [selected]

# [if needed] Wasabi – Access Keys create/download

##  configure cloud credentials
# add cloud account - http://px-backup-01.testco.local:31247/pxBackup/cloudSettings
# Cloud provier: AWS/SW Compliant Object Store
# Cloud account name: wasabi-px-testcolab-01
# Access key: [cloud provider access key]
# Secret key: [cloud provider secret key]

## configure a backup target location
# http://px-backup-01.testco.local:31247/pxBackup/addLocation
# name: px-testcolab-01
# Cloud account: wasabi-px-testcolab-01
# Path/Bucket: or-cluster-backups-01  # will create if one doesn't exist
# Encryption Key: [blank]
# region: us-west-1
# endpoint: s3.us-west-1.wasabisys.com
# disable SSL; [not selected]
# storage class: Standard

##  add cluster backup source to backup from and restore to
#  http://px-backup-03.testco.local:30122/pxBackup/clusterDiscovery/addCluster
#  Select Kubernetes Platform
#    Others [all cluster manually using kubeconfig]
# cluster name: or-cluster01
# [output from: kubectl config view --flatten --minify}
# Click Add Cluster

# drill into newly added cluster, confirm:
# applications
#   namespaces
#   object types

##  create a backup rule
# http://px-backup-01.testco.local:31247/pxBackup/rules
# http://px-backup-01.testco.local:31247/pxBackup/addBackupRule
# rule name: qed-backup
# pod selector: app=qed-static-blue
# container: [none]
# action: echo "qed"
#   background: [selected]


## create schedule policies
# http://px-backup-01.testco.local:31247/pxBackup/schedules
# policy name: qed-daily-01
# type: Daily
#   Hours: 01:00 AM
#   Retain: 7
#   Incremental Count: 6  #  [PX Volumes only]

## create and restore backups

## PX-Backup Uninstall
helm delete px-central --namespace central
kubectl delete namespace central
